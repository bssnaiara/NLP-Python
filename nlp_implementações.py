# -*- coding: utf-8 -*-
"""NLP_Implementações

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1erayO0unUH1Vch8sU-u-U8G0eaLKutAF

---

#Redes Neurais Recorrentes e Embeddings
Implementações de Modelos 
---
"""

#Instalando Bibliotecas
!pip install gensim
# !pip install fsspec
!git clone https://github.com/lucasvbalves/nlp-pt-br-datasets.git
!pip install nltk==3.6.2
# !pip install spacy==3.1.2

#Importando Bibliotecas
import nltk
import pandas as pd
import io
import re
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.layers import *
from keras.preprocessing import text, sequence
import numpy as np
import tensorflow
from tensorflow import keras
import matplotlib.pyplot as plt
import seaborn as sns
from tensorflow.keras.utils import plot_model
from tensorflow.keras.models import Model
from datetime import datetime as dt
from sklearn.metrics import classification_report
from collections import Counter
from nltk.tokenize import sent_tokenize, word_tokenize
from sklearn.model_selection import train_test_split
import gensim
from sklearn import preprocessing
from tensorflow.keras import utils
from gensim.models import Word2Vec
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from sklearn.metrics import confusion_matrix
from tensorflow.keras.layers import Dense,Input, BatchNormalization

"""###Etapa 1 - Geração do dataset"""

#Importações dos comentários negativos e positivos 

negativos = '/content/nlp-pt-br-datasets/books-reviews-portuguese-master/books_pt_neg'
positivos = '/content/nlp-pt-br-datasets/books-reviews-portuguese-master/books_pt_pos'

# Leitura dos documentos em UTF-8
with open(positivos,'r',encoding='utf8') as positivos_utf:
  print('Positivo: ',positivos_utf.readlines()[1])

with open(negativos,'r',encoding='utf8') as negativos_utf:
  print('Negativo: ',negativos_utf.readlines()[1])

#Criando o DataFrame
negativos_df = pd.read_csv('/content/nlp-pt-br-datasets/books-reviews-portuguese-master/books_pt_neg', sep='\r', names=['Comentarios', 'label'])
positivos_df = pd.read_csv('/content/nlp-pt-br-datasets/books-reviews-portuguese-master/books_pt_pos', sep='\r', names=['Comentarios', 'label'])

# Adicionando as labels negativas
negativos_df['label'] = negativos_df['label'].replace(np.nan, 'negativo')

#Exibindo o dataset negativo com as labels
negativos_df

# Adicionando as labels positivas
positivos_df['label'] = positivos_df['label'].replace(np.nan, 'positivo')
#Exibindo o dataset positivo com as labels
positivos_df

#Concatenando os comentários positivos e negativos 
dfp = pd.DataFrame(data=positivos_df)
dfn = pd.DataFrame(data=negativos_df)
df_comments = pd.concat([dfp, dfn], axis=0, ignore_index=True, names=['Comentarios', 'label'])
df_comments

#Exibindo informações do dataframe
df_comments.info()

#Mostrando se há comentários negativos
Counter(df_comments['Comentarios'].isna())

X = df_comments['Comentarios']
# Realizando a filtragem dos comentários
words = []
for i in range(len(X)):
  words.append(re.sub(r'[^\w]+', ' ',X[i]).lower())
X2 = pd.DataFrame(words, columns=['Comentarios'])

#Convertendo os comentários para lista (ou numpy array)
X = X2['Comentarios'].to_numpy()
X

#Verificando a quantidade de labels
classes = df_comments['label'].nunique()
classes

# Convertendo as labels para lista
y = df_comments['label'].to_numpy()
y

"""#Word2Vec"""

# Preparando os tokens para o treinamento do Word2Vec
sentences = [[word for word in document.lower().split()] for document in X]
print(len(sentences))

#Aplicando o word2vec
word_model = gensim.models.Word2Vec(sentences, size=300, min_count = 1, window = 5, iter=100)

# Pesos do treinamento do Word2vec
pretrained_weights = word_model.wv.syn0
vocab_size, emdedding_size = pretrained_weights.shape

"""# LSTM"""

#Divisão dos dados
X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size = 0.15, stratify = y)

#Salvando os comentários de teste
X_test_df = pd.DataFrame(X_test, columns=['Comentários'])

#Tokenização
tokenization = text.Tokenizer(num_words=10000,lower=True)

#Passando os textos para sequencias numéricas
tokenization.fit_on_texts(list(X_train)+list(X_test))
X_train = tokenization.texts_to_sequences(X_train)
X_test = tokenization.texts_to_sequences(X_test)
#Definindo um tamanho fixo máximo das sequências
x_train = sequence.pad_sequences(X_train, maxlen=300, padding="post", truncating="post")
x_test = sequence.pad_sequences(X_test, maxlen=300, padding="post", truncating="post")

#Vocabulário
word_index = tokenization.word_index
word_index
size_of_vocabulary = len(tokenization.word_index) + 1 #+1 for padding

#To categorical nos y's
num_classes = 2

y_train = np.where(y_train == "positivo", 1, 0)
y_test = np.where(y_test == "positivo", 1, 0)
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)

#Tokenizando os comentários
sentences = df_comments["Comentarios"].values.tolist()
tokenizer = Tokenizer()

tokenizer.fit_on_texts(sentences)

word_index = tokenizer.word_index
no_of_vocab = len(word_index)

#Tamanho do vocabulário
print(no_of_vocab)

#Tokens
print(word_index)

x_train[0]

# Construir o modelo convlucional LSTM, usando os rótulos auxiliares como perda para mesclar as informações
def create_model(no_of_vocab, embedding_dimension, word_index, embedding_matrix=0):
    words = Input(shape=(x_train.shape[1:])) 

    x = Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[pretrained_weights], trainable=True)(words)
    x = Conv1D(128, 3)(x)
    x = Conv1D(128, 3)(x)
    x = Conv1D(64, 3)(x)
    x = LSTM(128, return_sequences=True)(x)
    x = LSTM(64, return_sequences=True)(x)
    x = LSTM(32, return_sequences=False)(x)
    x = layers.Dropout(0.2)(x)

    x = layers.Dense(128, activation = 'relu')(x)
    x = layers.Dropout(0.2)(x)

    x = layers.Dense(64, activation = 'relu')(x)
    x = layers.Dropout(0.2)(x)

    x = layers.Dense(32, activation = 'relu')(x)

    x = layers.Dropout(0.2)(x)

    aux_result = Dense(2, activation='softmax')(x)

    model = Model(inputs=words, outputs=[aux_result])
    model.summary()

    return model


#Criação do modelo
model = create_model(no_of_vocab, 300, word_index)

#Compile do modelo
model.compile(loss='binary_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), metrics=['accuracy'],)

#Treinamento
print("Training Start")
start_time = dt.now()


history = model.fit(x_train, y_train, epochs=200, batch_size=32,
                    validation_split=.20, verbose=1,
                    callbacks=keras.callbacks.EarlyStopping(patience=5))

print("Training Complete, time elapsed =", dt.now()-start_time)

plt.figure(figsize=(10,8))
plt.title(f"Gráfico de Loss")
plt.xlabel("Épocas")
plt.ylabel("Valor")
plt.plot(history.history['loss'], label="loss")
plt.plot(history.history['val_loss'], label="val_loss")
plt.legend()
plt.show()

plt.figure(figsize=(10,8))
plt.title(f"Gráfico de Accuracy")
plt.xlabel("Épocas")
plt.ylabel("Valor")
plt.plot(history.history['accuracy'], label="accuracy")
plt.plot(history.history['val_accuracy'], label="val_accuracy")
plt.legend()
plt.show()

#Predict do modelo
y_pred = np.argmax(model.predict(x_test), axis=1)

#Matriz de Confusão da classificação do modelo
cm = confusion_matrix(y_test.argmax(axis=1), y_pred, normalize='true')
sns.heatmap(cm, annot=True, cmap='Blues_r')

#Outras métricas de avaliação
print(classification_report(y_test.argmax(axis=1), y_pred))

#Criando dataframes para os comentários de teste e seus respectivos predicts
df_pred = pd.DataFrame(data=X_test_df)
df_pred_y = pd.DataFrame(data=y_pred, columns=['Predict'])

#Concatenando os datasets
df_predict = pd.concat([df_pred, df_pred_y], axis=1, ignore_index=True)
df_predict.columns = ['Comentarios', 'Predict']
df_predict

#Mostrando algumas classificações
for i in range(0,11):
  print('Comentário', i,':', df_predict['Comentarios'][i])
  print('Predict', i,':', df_predict['Predict'][i])

#Criando um comentário negativo
frase = ['Esse livro é péssimo, me arrependo de pagar 56 reais nisso']
frase = tokenization.texts_to_sequences(frase)
frase_pad = sequence.pad_sequences(frase,maxlen= 300)

#Classificando o comentário criado
#Classe 0 = Positivo
#Classe 1 = Negativo
frase_pred = model.predict(frase_pad)
frase_pred.argmax()

#Criando um comentário positvo
frase2 = ['Me impressionei bastante com essa obra. Achava que ia ser mais um clichê entediante porem acabou sendo muito bom e divertido']
frase2 = tokenization.texts_to_sequences(frase2)
frase2_pad = sequence.pad_sequences(frase2,maxlen= 300)

#Classificando o comentário criado
frase_pred2 = model.predict(frase2_pad)
frase_pred2.argmax()

#Salvando o modelo
model.save('LSTM_model.h5')

"""#GRU"""

# Realizando a divisão dos dados de treino e teste 
# O tamanho do conjunto de teste é definido com 25% do total de dados
X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, test_size = 0.25)

#Salvando os comentários de teste
X_test_df = pd.DataFrame(X_test, columns=['Comentários'])

print('X_train dimension:', X_train.shape)
print('y_train dimension:', y_train.shape)
print('X_test dimension:', X_test.shape)
print('y_test dimension:', y_test.shape)

#Realizando a tokenização 
qnt_token = 30000
max_sequence_length = 100
tokenizer = Tokenizer(num_words = qnt_token)

#Criando um vocabulário com os texto do treinamento
tokenizer.fit_on_texts(list(X_train))

#Convertendo os textos para uma sequência de inteiros
X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)
         
#Fixando o tamanho da sequência em um tamanho único
X_train_pad  = pad_sequences(X_train_seq, maxlen = max_sequence_length)
X_test_pad = pad_sequences(X_test_seq, maxlen = max_sequence_length)

#Novo tamanho dos dados de treino
X_train_pad.shape

word_index = tokenizer.word_index
size_of_vocabulary = len(tokenizer.word_index) + 1 #+1 for padding
size_of_vocabulary

# Exibindo o tamanho do vocabulário
size_of_vocabulary

# Utilizando LabelEncoder para converter as labels para inteiros
encoder = preprocessing.LabelEncoder()
encoder.fit(list(y_train) + list(y_test)) 
y_train = encoder.transform(y_train)
y_test = encoder.transform(y_test)

# Binarizando as classes
num_classes = np.max(y_train) + 1
y_train = utils.to_categorical(y_train, num_classes)
y_test = utils.to_categorical(y_test, num_classes)

print('New dimensions')
print('X_train dimension:', X_train.shape)
print('y_train dimension:', y_train.shape)
print('X_test dimension:', X_test.shape)
print('y_test dimension:', y_test.shape)

# Criação do modelo de GRU
GRU = keras.Sequential()

# Camada de Embedding 
GRU.add(layers.Embedding(size_of_vocabulary, 200,
                        input_length = X_train_pad.shape[1],
                        trainable = False)) 
GRU.add(layers.SpatialDropout1D(0.3))
GRU.add(layers.GRU(200, recurrent_dropout = 0.3, return_sequences = True))
GRU.add(layers.GRU(200, recurrent_dropout = 0.3))
GRU.add(layers.Dense(512, activation = 'relu'))

GRU.add(layers.Dense(128, activation = 'relu')) 

GRU.add(layers.Dense(64, activation = 'relu')) 

GRU.add(layers.Dense(32, activation = 'relu'))

GRU.add(layers.Dense(num_classes, activation = 'softmax'))

GRU.summary()
plot_model(GRU)

# Definição dos callbacks
callback_earlier = keras.callbacks.EarlyStopping(patience=5)
reduce_lr =  keras.callbacks.ReduceLROnPlateau(patience=5)
callback = [callback_earlier, reduce_lr ]

GRU.compile(optimizer = 'adamax', 
                    loss = 'binary_crossentropy',
                    metrics = ['accuracy'])

#Treinamento
GRU_history = GRU.fit(X_train_pad, y_train, batch_size = 184, epochs = 50, validation_split=0.25, callbacks=callback)

# Avaliação do modelo
GRU.evaluate(X_test_pad, y_test)

plt.figure(figsize=(10,8))
plt.title(f"Gráfico de Loss")
plt.xlabel("Épocas")
plt.ylabel("Valor")
plt.plot(GRU_history.history['loss'], label="loss")
plt.plot(GRU_history.history['val_loss'], label="val_loss")
plt.legend()
plt.show()

plt.figure(figsize=(10,8))
plt.title(f"Gráfico de Accuracy")
plt.xlabel("Épocas")
plt.ylabel("Valor")
plt.plot(GRU_history.history['accuracy'], label="accuracy")
plt.plot(GRU_history.history['val_accuracy'], label="val_accuracy")
plt.legend()
plt.show()

y_pred = np.argmax(GRU.predict(X_test_pad), axis=1)
y_test = np.argmax(y_test, axis=1)

print(classification_report(y_test, y_pred))

#Matriz de Confusão da classificação do modelo
cm = confusion_matrix(y_test, y_pred, normalize='true')
sns.heatmap(cm, annot=True, cmap='Blues_r')

#Resetando o Index da variável de teste 
X_test_df = X_test_df.reset_index(drop=True)
X_test_df

#Criando dataframes para os comentários de teste e seus respectivos predicts
df_pred = pd.DataFrame(data=X_test_df)
df_pred_y = pd.DataFrame(data=y_pred, columns=['Predict'])

#Concatenando os datasets
df_predict = pd.concat([df_pred, df_pred_y], axis=1, ignore_index=True)
df_predict.columns = ['Comentarios', 'Predict']
df_predict

#Mostrando algumas classificações
for i in range(0,11):
  print('Comentário', i,':', df_predict['Comentarios'][i])
  print('Predict', i,':', df_predict['Predict'][i])

#Criando um comentário negativo
frase = ['Esse livro é péssimo, me arrependo de pagar 56 reais nisso']
frase = tokenizer.texts_to_sequences(frase)
frase_pad = sequence.pad_sequences(frase,maxlen= 100)

#Classificando o comentário criado
#Classe 0 = Negativo
#Classe 1 = Positivo
frase_pred = GRU.predict(frase_pad)
frase_pred.argmax()

#Criando um comentário positvo
frase2 = ['Me impressionei bastante com essa  obra. Achava que ia ser mais um clichê entediante porem acabou sendo muito bom e divertido']
frase2 = tokenizer.texts_to_sequences(frase2)
frase2_pad = sequence.pad_sequences(frase2,maxlen= 100)

#Classificando o comentário criado
frase_pred2 = GRU.predict(frase2_pad)
frase_pred2.argmax()

#Salvando o modelo
GRU.save('GRU_model.h5')

"""#LSTM Bidirecional """

#Divisão dos dados
X_train, X_test, y_train, y_test = train_test_split(X,y, stratify=y, shuffle=True, test_size=0.3)

#Salvando os comentários de teste
X_test_df = pd.DataFrame(X_test, columns=['Comentários'])

#Definindo um Tokenizer
tokenization = text.Tokenizer(num_words=10000,lower=True)

#Transformando os X's em sequencias numéricas
tokenization.fit_on_texts(list(X_train)+list(X_test))
X_train = tokenization.texts_to_sequences(X_train)
X_test = tokenization.texts_to_sequences(X_test)
x_train = sequence.pad_sequences(X_train, maxlen=300)
x_test = sequence.pad_sequences(X_test, maxlen=300)

#To categorical para os y's
y_train = np.where(y_train == "positivo", 1, 0)
y_test = np.where(y_test == "positivo", 1, 0)
y_train = tf.keras.utils.to_categorical(y_train, 2)
y_test = tf.keras.utils.to_categorical(y_test,2)

#Modelo LSTM Bidirectional
inputs = Input(shape=(300,))
x = inputs
x = Embedding(input_dim=vocab_size, output_dim=emdedding_size, weights=[pretrained_weights], trainable = False)(x)
x = SpatialDropout1D(0.2)(x)
x = Bidirectional(LSTM(200))(x)
x = Dropout(0.2)(x)
out = Dense(2, activation='softmax')(x)

Bidirectional_model = keras.Model(inputs, out, name='Model')

#Sumamry do modelo
Bidirectional_model.summary()

#Plotagem do modelo
plot_model(Bidirectional_model, show_shapes=True)

#Compile
Bidirectional_model.compile(loss='binary_crossentropy', optimizer='adam', metrics='accuracy')

#Limpar Backend
tf.keras.backend.clear_session()

#Treinamendo do modelo
hist = Bidirectional_model.fit(x_train, y_train,
                 epochs = 50, 
                 batch_size = 100,
                 validation_split=0.20,
                 verbose=1,
                 callbacks=[keras.callbacks.EarlyStopping(patience=3), keras.callbacks.ReduceLROnPlateau()])

# Evaluate do modelo
scores = Bidirectional_model.evaluate(x_test, y_test, verbose=0)

# Gráfico de loss
plt.figure(figsize=(10,8))
plt.title(f"Gráfico de Loss - {scores[0]}")
plt.xlabel("Épocas")
plt.ylabel("Valor")
plt.plot(hist.history['loss'], label="loss")
plt.plot(hist.history['val_loss'], label="val_loss")
plt.legend()
plt.show()

# Gráfico de acurácia
plt.figure(figsize=(10,8))
plt.title(f"Gráfico de Acurácia - {scores[1]}")
plt.xlabel("Épocas")
plt.ylabel("Valor")
plt.plot(hist.history['accuracy'], label="accuracy")
plt.plot(hist.history['val_accuracy'], label="val_accuracy")
plt.legend()
plt.show()

#Predict do modelo
pred = Bidirectional_model.predict(x_test)
pred = np.round(pred)

#Matriz de confusão
cm = confusion_matrix(y_test.argmax(axis=1), pred.argmax(axis=1), normalize='true')
sns.heatmap(cm, annot=True, cmap='Blues_r')

#Outras métricas
print(classification_report(y_test.argmax(axis=1), pred.argmax(axis=1)))

#Criando dataframes para os comentários de teste e seus respectivos predicts
df_pred = pd.DataFrame(data=X_test_df)
df_pred_y = pd.DataFrame(data=pred.argmax(axis=1), columns=['Predict'])

#Concatenando os datasets
df_predict = pd.concat([df_pred, df_pred_y], axis=1, ignore_index=True)
df_predict.columns = ['Comentarios', 'Predict']
df_predict

#Mostrando algumas classificações
for i in range(0,11):
  print('Comentário', i,':', df_predict['Comentarios'][i])
  print('Predict', i,':', df_predict['Predict'][i])

#Criando um comentário negativo
frase = ['Esse livro é péssimo, me arrependo de pagar 56 reais nisso']
frase = tokenization.texts_to_sequences(frase)
frase_pad = sequence.pad_sequences(frase,maxlen= 300)

#Classificando o comentário criado
#Classe 0 = Negativo
#Classe 1 = Positivo
frase_pred = Bidirectional_model.predict(frase_pad)
frase_pred.argmax()

#Criando um comentário positvo
frase2 = ['Me impressionei bastante com essa obra. Achava que ia ser mais um clichê entediante porem acabou sendo muito bom e divertido']
frase2 = tokenization.texts_to_sequences(frase2)
frase2_pad = sequence.pad_sequences(frase2,maxlen= 300)

#Classificando o comentário criado
frase_pred2 = Bidirectional_model.predict(frase2_pad)
frase_pred2.argmax()

#Salvando o modelo
model.save('Bidirectional_model.h5')